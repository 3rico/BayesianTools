---
title: "Reference manual"
author: "Florian Hartig"
date: "10 Jan 2016"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Describes the detailed options for the samplers}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8](inputenc)
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=5, warning=FALSE, message=FALSE)
```


```{r, echo = F, message = F}
set.seed(123)
library(BayesianTools)
```

# Possibilities of the package

The BT package supports model analysis (including sensitivity analysis and uncertainty analysis), Bayesian model calibration, as well as model selection and multi-model inference techniques.



# The bayesianSetup


The purpose of this document is to provide a better insight in the options of the Metropolis sampler. We will use a simple 3-d multivariate normal density for this demonstration. 

```{r}
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
```

## Creating a likelihood 

The likelihood should be provided as a log density function. See options for parallelization below.


## Creating a prior 

Prior consists of four parts

* A log density function
* A sampling function (must be a function without parameters, that returns a draw from the prior)
* lower / upper boundaries
* Additional info - best values, names of the parameters, ... 

These information can passed by first creating an a extra object, via createPrior, or through the the createBayesianSetup function. 

Creating an extra prior is useful in particular because some pre-defined priors are available. See ?createPrior for a list.


## Parallelization of the likelihood evaluations

Algorithms in the BayesianTools package can make use of parallel computing if this option is specified in the BayesianSetup. Note that currently, parallelization is used by the following algorithms: SMC, DE and DREAM sampler. It can also be used through the BayesianSetup with the functions of the sensitivity package. 

There are two ways to use parallelization with the "parallel" argument in the createBayesianSetup() function.

#### 1. In-build parallelization:

The in-build parallelization is the easiest way to make use of parallel computing.  In the "parallel" argument you can choose the number of cores used for parallelization. Alternatively for TRUE or "auto" all available cores except for one will be used. Now the proposals are evaluated in parallel. Technically, the in-build parallelization uses an R cluster to evaluate the posterior density function. The input for the parallel function is a matrix, where each column represents a parameter and each row a proposal. In this way, the proposals can be evaluated in parallel. For sampler, where only one proposal is evaluated at a time (namely the Metropolis based algorithms), no parallelization can be used.

#### 2. External parallelization

The second option is to use an external parallelization. Here, a parallelization is attempted in the user defined likelihood function. To make use of external parallelization, the likelihood function needs to take a matrix of proposals and return a vector of likelihood values. In the proposal matrix each row represents one proposal, each column a parameter. Further, you need to specify the "external" parallelization in the "parallel" argument. In simplified terms the use of external parallelization uses the following steps:

```{r, eval = FALSE}
## Definition of likelihood function
likelihood <- function(matrix){
	# Calculate likelihood in parallel
	# Return vector of likelihood valus
}

## Create Bayesian Setup
BS <- createBayesianSetup(likelihood, parallel = "external" ...)

## Run MCMC
runMCMC(BS, sampler = "SMC", ...)
```

** Remark: even though parallelization can significantly reduce the computation time, it is not always useful because of the so-called communication overhead (computational time for distributing and retrieving infos from the parallel cores). For models with low computational cost, this procedure can take more time than the actual evaluation of the likelihood. If in doubt, make a small comparison of the runtime before starting your large sampling. **

# The MCMC samplers

## runMCMC

The runMCMC function is the central function for starting MCMC algorithms in the BayesianTools package. It requires a bayesianSetup, a choice of sampler (standard is DEzs), and optionally changes to the standard settings of the chosen sampler.

runMCMC(bayesianSetup, sampler = "DEzs", settings = NULL)




One optional argument that you can always use is nrChains - the default is 1. If you choose more, the runMCMC will perform several runs.

```{r}

ll <- generateTestDensityMultiNormal(sigma = "no correlation")

bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))

settings = list(iterations = 10000, nrChains= 3, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

plot(out)
marginalPlot(out)
correlationPlot(out)
gelmanDiagnostics(out, plot=T)

# option to restart the sampler

settings = list(iterations = 1000, nrChains= 1, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

out2 <- runMCMC(bayesianSetup = out)

out3 <- runMCMC(bayesianSetup = out2)

plot(out)
plot(out3)

# create new prior from posterior sample 

newPriorFromPosterior <- createPriorDensity(out2)

```


## The different samplers

For convenience we define a number of iterations

```{r}
iter = 10000
```

## Metropolis Algorithms

The BayesianTools package comprises a number of Metropolis-Hastings (MH) based algrithms. All of these samplers can be accessed by the "Metropolis" sampler in the runMCMC function by specifying the sampler's settings.

The following code gives an overview about the default settings of the MH sampler.
```{r}
applySettingsDefault(sampler = "Metropolis")
```

The following examples show how the different settings can be used. As you will see different options can be activated singlely or in combination. 

### Standard MH MCMC

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = F, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = F, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Standard MH MCMC, prior optimization

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = F, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```

### Adaptive MCMC, prior optimization, delayed rejection

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 2, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization, gibbs updating

Currently adaptive cannot be mixed with gibbs updating

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = c(1,1,0), temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization, gibbs updating, tempering

```{r, results = 'hide'}
temperingFunction <- function(x) 5 * exp(-0.01*x) + 1
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = c(1,1,0), temperingFunction = temperingFunction, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```



## Other MCMCs


#### Differential Evolution MCMC
```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DE", settings = settings)
plot(out) 
```

#### Differential Evolution MCMC with snooker update

```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DEzs", settings = settings)
plot(out) 
```

#### DREAM sampler
```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAM", settings = settings)
plot(out) 
```

#### DREAM sampler with snooker update
```{r, results = 'hide', eval = FALSE}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAMzs", settings = settings)
#plot(out) 
```


#### DREAM sampler with snooker update
```{r, results = 'hide', eval = FALSE}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Twalk", settings = settings)
#plot(out) 
```

## Non-MCMC sampling algorithms 

### Rejection samling
```{r, results = 'hide'}
settings <- list(initialParticles = iter, iterations= 1)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "SMC", settings = settings)
plot(out) 
```



### Sequential Monte Carlo (SMC)
```{r, results = 'hide'}
settings <- list(initialParticles = iter, iterations= 10)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "SMC", settings = settings)
plot(out) 
```


## Convergence checks for MCMCs

```{r, results = 'hide'}
settings <- list(iterations = iter, nrChains = 3, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out)

library(coda)

chain = getSample(out, coda = T)


```


# Model comparison and averaging


## Predictive model selection criteria 


## Bayes factor



## Example



Data linear Regression with quadratic and linear effect

```{r}
sampleSize = 30
x <- (-(sampleSize-1)/2):((sampleSize-1)/2)
y <-  1 * x + 1*x^2 + rnorm(n=sampleSize,mean=0,sd=10)
plot(x,y, main="Test Data")
```

Likelihoods for both

```{r}
likelihood1 <- function(param){
    pred = param[1] + param[2]*x + param[3] * x^2
    singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[4]^2), log = T)
    return(sum(singlelikelihoods))  
}

likelihood2 <- function(param){
    pred = param[1] + param[2]*x 
    singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[3]^2), log = T)
    return(sum(singlelikelihoods))  
}
```


Posterior definitions


```{r}
setUp1 <- createBayesianSetup(likelihood1, lower = c(-5,-5,-5,0.01), upper = c(5,5,5,30))

setUp2 <- createBayesianSetup(likelihood2, lower = c(-5,-5,0.01), upper = c(5,5,30))
```

MCMC and marginal likelihood calculation

```{r, results = 'hide'}
settings = list(iterations = 15000, consoleUpdates = 1e+8)
out1 <- runMCMC(bayesianSetup = setUp1, sampler = "Metropolis", settings = settings)
tracePlot(out1, start = 5000)
M1 = marginalLikelihood(out1)
M1

settings = list(iterations = 15000, consoleUpdates = 1e+8)
out2 <- runMCMC(bayesianSetup = setUp2, sampler = "Metropolis", settings = settings)
tracePlot(out2, start = 5000)
M2 = marginalLikelihood(out2)
M2
```

### Model comparison via Bayes factor

Bayes factor (need to reverse the log)

```{r}
exp(M1$ln.m - M2$ln.m)
```

BF > 1 means the evidence is in favor of M1. See Kass, R. E. & Raftery, A. E. (1995) Bayes Factors. J. Am. Stat. Assoc., Amer Statist Assn, 90, 773-795.


### Model comparison via DIC

The Deviance information criterion is a commonly applied method to summarize the fit of an MCMC chain. It can be obtained via

```{r}
DIC(out1)$DIC
DIC(out2)$DIC
```
 




