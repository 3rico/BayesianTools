---
title: "Bayesian Tools"
author: "Florian Hartig"
date: "10 Jan 2016"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Manual for the BayesianTools R package}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8](inputenc)
abstract: "The 'BayesianTools' R package contains various MCMC and SMC samplers, plots and diagnostics for Bayesian analysis, with a particular focus on calibrating system models.\n \n \n"

---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=5, warning=FALSE)
```

```{r, echo = F, message = F}
set.seed(123)
library(BayesianTools)
```



The BT package supports model analysis (including sensitivity analysis and uncertainty analysis), Bayesian model calibration, as well as model selection and multi-model inference techniques.

A more detailed tutorial on Bayesian inference with process-based models available [here](https://www.dropbox.com/s/y3d5fqh61eqdzz6/TG13-ModelCalibrationTutorial.pdf?dl=0). The tutorial also explains in detail the Bayesian philosophy, and issues such as Interpretation of parameter estimates and confidence intervals


# Quick start

The purpose of this first section is to give you a quick overview of the most important functions of the BayesianTools package. For a more detailed description, see the later sections

## The Bayesian Setup

One central object in the package is the BayesianSetup. It contains the information about the model to be fit (likelihood), and the priors for the model parameters. 

A BayesianSetup is created throug the createBayesianSetup function. The function expects a log-likelihood and log-prior. It calculate automatically the posterior and various convenience functions for the samplers. 

Advantages of the BayesianSetup include 1) support for parallelization, 2) functions are wrapped in try-catch statements to avoid crashes during long MCMC evaluations, 3) and the posterior checks if the parameter is outside the prior first, in which case the likelihood is not evaluated (makes the algorithms faster for slow likelihoods).

If no prior information at all is provided, an infinite flat prior is created. If no explicit prior, but lower and upper values are provided, a standard uniform prior is created, including the option to sample from this prior, which is useful for SMC and also for getting starting values. This option is used in the follwoing example, which creates a multivariate normal likelihood density and a uniform prior for 3 parameters.

```{r}
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
```

If you want to use your own custom functions, note the following

### Likelihood definition

As likelihood function, you need to provide the log likelihood density of a parameter vector 

```{r, eval = F}
ll = logDensity(x)
```

Likelihoods are often costly to compute. If that is the case for you, you should think about parallelization possibilities. The BayesianSetup has the following options via the variable parallel

* F means no parallelization
* T means that an automatic parallelization via R is attempted (will not work if your likelihood writes to file, or uses global variables or functions - see general R help on parallelization) 
* "external", assumed that the likelihood is already parallelized. In this case it needs to accept a matrix with parameters as columns. This is for using complex models that cannot be parallelized via an R cluster, e.g. because they write to file. 

### Prior definition

You have 3 options to set priors

* Only set min/max values in the createBayesianSetup, or even drop that (creates flat priors)
* Use one of the predefinded priors, see ?createPrior for a list. One of the options here is to use a previous MCMC output as new prior
* Use a user-define prior, see ?createPrior 

In principle, the following information can be provided to priors:

* A log density function, as a function of a parameter vector x, as in the likelihood
* Additinally, you should consider providing a function that samples from the prior, because many samplers (SMC, DE, DREAM) can make use of this function for initial conditions. For this purpose, there is the argument sampler in the createBayesianSetup. If you use one of the pre-defined priors, the sampling function is already implemented
* lower / upper boundaries (can be set on top of any prior)
* Additional info - best values, names of the parameters, ... 

## Running MCMC and SMC functions

The runMCMC function is the main wrapper for all other implemented MCMC functions. It always takes the following arguments 

* A bayesianSetup (alternatively, the log target function)
* The sampler name 
* A list with settings - if a parameter is not provided, the default will be used

### The Metropolis MCMC

A versatile Metropolis-type MCMC with options for covariance adaptation, delayed rejection, tempering and Metropolis-within-Gibbs sampling is available via the Metropolis option. For details, see the help of the Metropolis file

```{r}
iter = 10000
settings = list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
```

All samplers can be plotted and summarized via the console

```{r}
print(out)
summary(out)
```

and plotted with the following plot functions

```{r}
plot(out) # plot internally calls tracePlot(out)
correlationPlot(out)
marginalPlot(out)
```

Other Functions that can be applied to all samplers include

```{r}
marginalLikelihood(out)
DIC(out)
MAP(out)
```

You can extract (a part of) the sampled parameter values by

```{r}
#getSample(out, start = 100, end = NULL, thin = 5, whichParameters = 1:2)
```


For all samplers, you can perform multiple runs via the nrChains argument


```{r}
iter = 1000
settings = list(iterations = iter, nrChains = 3, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

```

The result is an object of mcmcSamplerList, which should allow to do everything one can do with an mcmcSampler object (with slightly different output sometimes). 

```{r}
print(out)
summary(out)
marginalLikelihood(out)
DIC(out)
```


For example, in the plot you now see 3 chains. 

```{r}
plot(out)
```

There are a few additional functions that may only be available for lists, for example 

```{r}
#getSample(out, coda = F)
gelmanDiagnostics(out, plot = T)
```


### Other Metropolis MCMCs

Only for comparison, there are a numer of other Metropolis-type algorithms implemented. In priciple, either of these algorithms can be emulated via the options of the main Metropolis functions. We do not recommend using these algorithms except for comparisons. 

```{r}
settings = list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "M", settings = settings)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "AM", settings = settings)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DR", settings = settings)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DRAM", settings = settings)

plot(out)
correlationPlot(out)
marginalPlot(out)
summary(out)
```


### Differential Evolution (DE) samplers

There are two versions of the differentia evolution MCMC included. In doubt, you should use the DEzs option. 

```{r}
settings = list(iterations = iter, consoleUpdates = 1e+8)

out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DE", settings = settings)

out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DEzs", settings = settings)

```


```{r}
plot(out)
correlationPlot(out)
marginalPlot(out)
summary(out)
gelmanDiagnostics(out, plot = T)
```

### DREAM sampler family

Also for the DREAM sampler, there are two versions included. Again, you should usually use the "DREAMzs". 

```{r}
settings = list(iterations = iter, nseq = 6, consoleUpdates = 1e+8) # the rest of the parameters are default settings.

out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAM", settings = settings)

settings = list(iterations = iter, nseq = 3) # Snooker update allows for less chains.
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAMzs", settings = settings)
```

```{r}
plot(out)
correlationPlot(out)
marginalPlot(out)
summary(out)
gelmanDiagnostics(out, plot = T)
```

### T-walk

```{r}
settings = list(iterations = iter, consoleUpdates = 1e+8) # the rest of the parameters are default settings.

out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Twalk", settings = settings)
```

```{r}
plot(out)
correlationPlot(out)
marginalPlot(out)
summary(out)
#gelmanDiagnostics(out, plot = T)
```


### SMC sampler

```{r}
settings = list(initialParticles = 1000, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "SMC", settings = settings)

plot(out)
correlationPlot(out)
marginalPlot(out)
summary(out)

```

Note that the above settins initialParticles = 1000 requires that the bayesianSetup includes the possibility to sample from the prior. 


## Bayesian model selection and model averaging

The package provides functions to calculate DIC and the marginal Likelihood (for the calculation of the Bayesi factor. See Reference Manual for more details)


```{r}
settings = list(consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

DIC(out)

marginalLikelihood(out)
```


The DIC function provides further information, e.g. the effective number of parameters. The original reference is Spiegelhalter, D. J.; Best, N. G.; Carlin, B. P. & van der Linde, A. (2002) Bayesian measures of model complexity and fit. J. Roy. Stat. Soc. B, 64, 583-639.

An interesting reference DIC and other Bayesian information criteria is Gelman, A.; Hwang, J. & Vehtari, A. (2014) Understanding predictive information criteria for Bayesian models. Statistics and Computing, Springer US, 24, 997-1016-.



# BayesianSetup Reference


## Creating a likelihood 

The likelihood should be provided as a log density function. See options for parallelization below. We will use a simple 3-d multivariate normal density for this demonstration. 

```{r}
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
```

### Parallelization of the likelihood evaluations

Algorithms in the BayesianTools package can make use of parallel computing if this option is specified in the BayesianSetup. Note that currently, parallelization is used by the following algorithms: SMC, DE and DREAM sampler. It can also be used through the BayesianSetup with the functions of the sensitivity package. 

There are two ways to use parallelization with the "parallel" argument in the createBayesianSetup() function.

#### 1. In-build parallelization:

The in-build parallelization is the easiest way to make use of parallel computing.  In the "parallel" argument you can choose the number of cores used for parallelization. Alternatively for TRUE or "auto" all available cores except for one will be used. Now the proposals are evaluated in parallel. Technically, the in-build parallelization uses an R cluster to evaluate the posterior density function. The input for the parallel function is a matrix, where each column represents a parameter and each row a proposal. In this way, the proposals can be evaluated in parallel. For sampler, where only one proposal is evaluated at a time (namely the Metropolis based algorithms), no parallelization can be used.

#### 2. External parallelization

The second option is to use an external parallelization. Here, a parallelization is attempted in the user defined likelihood function. To make use of external parallelization, the likelihood function needs to take a matrix of proposals and return a vector of likelihood values. In the proposal matrix each row represents one proposal, each column a parameter. Further, you need to specify the "external" parallelization in the "parallel" argument. In simplified terms the use of external parallelization uses the following steps:

```{r, eval = FALSE}
## Definition of likelihood function
likelihood <- function(matrix){
	# Calculate likelihood in parallel
	# Return vector of likelihood valus
}

## Create Bayesian Setup
BS <- createBayesianSetup(likelihood, parallel = "external" ...)

## Run MCMC
runMCMC(BS, sampler = "SMC", ...)
```

** Remark: even though parallelization can significantly reduce the computation time, it is not always useful because of the so-called communication overhead (computational time for distributing and retrieving infos from the parallel cores). For models with low computational cost, this procedure can take more time than the actual evaluation of the likelihood. If in doubt, make a small comparison of the runtime before starting your large sampling. **


## Creating a prior 

Prior consists of four parts

* A log density function
* A sampling function (must be a function without parameters, that returns a draw from the prior)
* lower / upper boundaries
* Additional info - best values, names of the parameters, ... 

These information can passed by first creating an a extra object, via createPrior, or through the the createBayesianSetup function. 

Creating an extra prior is useful in particular because some pre-defined priors are available. See ?createPrior for a list.



# MCMC sampler reference

## runMCMC

The runMCMC function is the central function for starting MCMC algorithms in the BayesianTools package. It requires a bayesianSetup, a choice of sampler (standard is DEzs), and optionally changes to the standard settings of the chosen sampler.

runMCMC(bayesianSetup, sampler = "DEzs", settings = NULL)

One optional argument that you can always use is nrChains - the default is 1. If you choose more, the runMCMC will perform several runs.

```{r}

ll <- generateTestDensityMultiNormal(sigma = "no correlation")

bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))

settings = list(iterations = 10000, nrChains= 3, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

plot(out)
marginalPlot(out)
correlationPlot(out)
gelmanDiagnostics(out, plot=T)

# option to restart the sampler

settings = list(iterations = 1000, nrChains= 1, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)

out2 <- runMCMC(bayesianSetup = out)

out3 <- runMCMC(bayesianSetup = out2)

plot(out)
plot(out3)

# create new prior from posterior sample 

newPriorFromPosterior <- createPriorDensity(out2)

```


## The different samplers

For convenience we define a number of iterations

```{r}
iter = 10000
```

## Metropolis Algorithms

The BayesianTools package comprises a number of Metropolis-Hastings (MH) based algrithms. All of these samplers can be accessed by the "Metropolis" sampler in the runMCMC function by specifying the sampler's settings.

The following code gives an overview about the default settings of the MH sampler.
```{r}
applySettingsDefault(sampler = "Metropolis")
```

The following examples show how the different settings can be used. As you will see different options can be activated singlely or in combination. 

### Standard MH MCMC

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = F, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = F, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Standard MH MCMC, prior optimization

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = F, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```

### Adaptive MCMC, prior optimization, delayed rejection

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 2, gibbsProbabilities = NULL, temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization, gibbs updating

Currently adaptive cannot be mixed with gibbs updating

```{r, results = 'hide'}
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = c(1,1,0), temperingFunction = NULL, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```


### Adaptive MCMC, prior optimization, gibbs updating, tempering

```{r, results = 'hide'}
temperingFunction <- function(x) 5 * exp(-0.01*x) + 1
settings <- list(iterations = iter, adapt = T, DRlevels = 1, gibbsProbabilities = c(1,1,0), temperingFunction = temperingFunction, optimize = T, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out) 
```



## Other MCMCs


#### Differential Evolution MCMC
```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DE", settings = settings)
plot(out) 
```

#### Differential Evolution MCMC with snooker update

```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DEzs", settings = settings)
plot(out) 
```

#### DREAM sampler
```{r, results = 'hide'}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAM", settings = settings)
plot(out) 
```

#### DREAM sampler with snooker update
```{r, results = 'hide', eval = FALSE}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAMzs", settings = settings)
#plot(out) 
```


#### DREAM sampler with snooker update
```{r, results = 'hide', eval = FALSE}
settings <- list(iterations = iter, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Twalk", settings = settings)
#plot(out) 
```

## Non-MCMC sampling algorithms 

### Rejection samling
```{r, results = 'hide'}
settings <- list(initialParticles = iter, iterations= 1)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "SMC", settings = settings)
plot(out) 
```



### Sequential Monte Carlo (SMC)
```{r, results = 'hide'}
settings <- list(initialParticles = iter, iterations= 10)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "SMC", settings = settings)
plot(out) 
```


## Convergence checks for MCMCs

```{r, results = 'hide'}
settings <- list(iterations = iter, nrChains = 3, consoleUpdates = 1e+8)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
plot(out)

library(coda)

chain = getSample(out, coda = T)


```


# Model comparison and averaging


## Predictive model selection criteria 


## Bayes factor



## Example



Data linear Regression with quadratic and linear effect

```{r}
sampleSize = 30
x <- (-(sampleSize-1)/2):((sampleSize-1)/2)
y <-  1 * x + 1*x^2 + rnorm(n=sampleSize,mean=0,sd=10)
plot(x,y, main="Test Data")
```

Likelihoods for both

```{r}
likelihood1 <- function(param){
    pred = param[1] + param[2]*x + param[3] * x^2
    singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[4]^2), log = T)
    return(sum(singlelikelihoods))  
}

likelihood2 <- function(param){
    pred = param[1] + param[2]*x 
    singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[3]^2), log = T)
    return(sum(singlelikelihoods))  
}
```


Posterior definitions


```{r}
setUp1 <- createBayesianSetup(likelihood1, lower = c(-5,-5,-5,0.01), upper = c(5,5,5,30))

setUp2 <- createBayesianSetup(likelihood2, lower = c(-5,-5,0.01), upper = c(5,5,30))
```

MCMC and marginal likelihood calculation

```{r, results = 'hide'}
settings = list(iterations = 15000, consoleUpdates = 1e+8)
out1 <- runMCMC(bayesianSetup = setUp1, sampler = "Metropolis", settings = settings)
tracePlot(out1, start = 5000)
M1 = marginalLikelihood(out1)
M1

settings = list(iterations = 15000, consoleUpdates = 1e+8)
out2 <- runMCMC(bayesianSetup = setUp2, sampler = "Metropolis", settings = settings)
tracePlot(out2, start = 5000)
M2 = marginalLikelihood(out2)
M2
```

### Model comparison via Bayes factor

Bayes factor (need to reverse the log)

```{r}
exp(M1$ln.m - M2$ln.m)
```

BF > 1 means the evidence is in favor of M1. See Kass, R. E. & Raftery, A. E. (1995) Bayes Factors. J. Am. Stat. Assoc., Amer Statist Assn, 90, 773-795.


### Model comparison via DIC

The Deviance information criterion is a commonly applied method to summarize the fit of an MCMC chain. It can be obtained via

```{r}
DIC(out1)$DIC
DIC(out2)$DIC
```
 


